<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>VAEs and GANs | Zining&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="There are three most popular types of generative models out there: FVBN (Fully Visible Bayes Net), VAE (Variational Auto-Encoders), and GAN (Generative Adversal Networks). This blog talks about how VA">
<meta property="og:type" content="article">
<meta property="og:title" content="VAEs and GANs">
<meta property="og:url" content="https://ziningzhu.github.io/Blog/2017/05/14/2017_05_VAE_GAN/index.html">
<meta property="og:site_name" content="Zining&#39;s Blog">
<meta property="og:description" content="There are three most popular types of generative models out there: FVBN (Fully Visible Bayes Net), VAE (Variational Auto-Encoders), and GAN (Generative Adversal Networks). This blog talks about how VA">
<meta property="og:locale">
<meta property="article:published_time" content="2017-05-15T02:35:00.000Z">
<meta property="article:modified_time" content="2023-09-02T15:05:29.196Z">
<meta property="article:author" content="Zining Zhu">
<meta property="article:tag" content="VAE">
<meta property="article:tag" content="GAN">
<meta name="twitter:card" content="summary">
<meta name="twitter:creator" content="@https:&#x2F;&#x2F;twitter.com&#x2F;zhuzining">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-84222809-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  <meta name="google-site-verification" content="DI7qg6_y3mwWTdMA-GjrIY60uIS0beb12GxXZZ4A6gw" />

  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <script src="https://use.fontawesome.com/93f0a51ca6.js"></script>
<meta name="generator" content="Hexo 5.4.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Zining&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~zining">About</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a id="nav-twitter-link" class="nav-icon" target="_blank" rel="noopener" href="https://twitter.com/zhuzining">
        
        
          <a id="nav-linkedin-link" class="nav-icon" target="_blank" rel="noopener" href="https://www.linkedin.com/in/zining-zhu-49164496/">
        
        
          <a id="nav-github-link" class="nav-icon" target="_blank" rel="noopener" href="http://github.com/ZiningZhu">
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://ziningzhu.github.io/Blog"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-2017_05_VAE_GAN" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/05/14/2017_05_VAE_GAN/" class="article-date">
  <time datetime="2017-05-15T02:35:00.000Z" itemprop="datePublished">2017-05-14</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      VAEs and GANs
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>There are three most popular types of generative models out there: FVBN (Fully Visible Bayes Net), VAE (Variational Auto-Encoders), and GAN (Generative Adversal Networks). This blog talks about how VAE and GAN are set up, and briefly discusses their similarity.<br><span id="more"></span><br>One of the important tasks in AI and machine learning is inference, where we need to figure out how the distribution of a variable $z$ (model class) changes as influenced by the other variable, $x$ (observation). Such inference is based on the Bayes’ Rule</p>
<script type="math/tex; mode=display">p(z|x)=p(x|z)\frac{p(z)}{p(x)} = \frac{p(x|z)p(x)}{\int_z p(x|z)p(z)dz}</script><p>If we treat $p(x|z)$ as the likelihood of data given class, then the Bayes formula shows how $p(z|x)$, the “a posteriori” distribution, changes as new observation data accumulates. In other words, the Bayes formula shows how inference is passed from data variable $x$ to the latent class variable $z$. If the density is tractable, we can directly integrate it up, or, “pass a message from x to z”. Fully Visible Bayes Net works in this way.  </p>
<p>In reality, the integration term on the right hand side is hard to compute, so we want to find methods to reduce its complexity.<br>One of the ways to make the probability tractable is approximation. Let $q(z|x)$, a probability distribution function parameterized by $\phi$, approximate $p(z|x)$. By minimizing their difference we can use a easier-to-calculate PDF in place of an intractable one. Two metrics to measure the difference of probability distributions are Kullback-Leibler divergence and Jensen-Shannon divergence. Minimizing the former leads to VAE, and the latter leads to GAN.</p>
<p>Let’s first look at how to minimize <script type="math/tex">D_{KL}(q_\phi (z|x) || p(z|x))</script>. It is not easily computable, since <script type="math/tex">p(z|x)</script> is already intractable. Notice that we have the following equivalency:</p>
<script type="math/tex; mode=display">log p(x) - D_{KL}(q_\phi (z|x) || p(z|x)) = \int q_\phi (z|x) log \frac{p(x) p(z|x)}{q_\phi (z|x)} dz</script><script type="math/tex; mode=display">= \mathbb{E}_{q_\phi (z|x)} [log p(x|z) + log p(z) - log q_\phi (z|x)]</script><script type="math/tex; mode=display">= \mathbb{E}_{q_\phi (z|x)} [log p(x|z)] - D_{KL}(q_\phi (z|x) || p(z))</script><script type="math/tex; mode=display">= \mathbb{E}_{q_\phi (z|x)} [log p(x|z) + log p(z)] + \mathbb{H}[q_\phi (z|x)]</script><p>The right hand side of above equations are called Evidence Lower BOund (ELBO). For a given prior $p(x)$, maximizing the ELBO minimizes <script type="math/tex">D_{KL}(q_\phi (z|x)||p(z|x))</script>.</p>
<p>In order to optimize ELBO, we need the numerical values of $\frac{\partial \mathcal L}{\partial \theta}$ and $\frac{\partial \mathcal L}{\partial \phi}$. With tools like Tensorflow, Theano and Autograd, we hope to be able to automatically do that if ELBO consists of either continuous or discrete functions. However, we need to make sure such numerical optimization is doable theoretically.  </p>
<p>Problems still exist. First, only in some conditions (e.g., p(z|x) belongs to exponential family) that the KL term in ELBO has a closed analytical form.  </p>
<p>Second, the remaining term, <script type="math/tex">\mathbb{E}_{q_\phi (z|x)} [log p(x|z)]</script>, is an integration over a probability distribution function: <script type="math/tex">\int q_\phi (z|x) log(p(x|z)) dz</script>, where z obeys the distribution $p(z|x)$. To numerically calculate the integral, we need to sample z from $p(z|x)$, which is intractible. Deadlock! How to acquire $z$ then?  </p>
<p>(Kingma and Wellings, 2014)<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> puts forward the “vanilla” variational encoder, which involves setting up a SGVB estimator to acquire $z$ by acquiring <script type="math/tex">\tilde{z}</script>, where <script type="math/tex">z \sim q_\phi (z|x)</script> and</p>
<script type="math/tex; mode=display">\tilde{z} = g_\phi (\epsilon, x), \epsilon \sim p(\epsilon)</script><p>This is a reparameterization (approximation) from a probability distribution, <script type="math/tex">q_\phi (z|x)</script>, to a continuous, differentiable function, <script type="math/tex">g_\phi (\epsilon, x)</script>, which makes the ELBO optimizable. In this model, <script type="math/tex">q_\phi (z|x)</script> is similar to an encoder that “encodes” data into latent variable, and <script type="math/tex">p_\theta (x|z)</script> works like a decoder that “decodes” latent information. Its name Variational Autoencoder reflects this feature.</p>
<p>How to select the function $g(.)$ and latent variable prior <script type="math/tex">p(\mathbf{\epsilon})</script> though? The VAE<sup id="fnref:1"><a href="#fn:1" rel="footnote">1</a></sup> paper suggested a table of corresponding reparameterization function $g(.)$ and the assumed distribution <script type="math/tex">q_\phi (z|x)</script>.</p>
<p>There have been other efforts trying to make ELBO optimizable. SVI<sup id="fnref:2"><a href="#fn:2" rel="footnote">2</a></sup> uses Markov Chain Monte Carlo sampling from $q_\phi (z|x)$ to approximate values. ADVI<sup id="fnref:3"><a href="#fn:3" rel="footnote">3</a></sup> first transforms the model into one with unconstrained real-valued latent variables, and then perform elliptical standardization, after which the ELBO is differentiable.</p>
<p>Above is the story of variational autoencoder. In the training of a VAE model, an encoder and a decoder (both implemented as neural networks). Let <script type="math/tex">q_\phi (z|x)</script> and <script type="math/tex">p_\theta (x|z)</script> denote the probabilities output by the encoder and decoder neural networks, then the training of VAE minimizes <script type="math/tex">D_{KL} (q || p)</script>. However, it seems like there is no specific “purpose” of <script type="math/tex">q_\phi (z|x)</script> and <script type="math/tex">p_\theta (x|z)</script>: what does it mean to map the data $x$ to the latent space $z$?</p>
<p>One way to think about their purpose is to let the <script type="math/tex">p_\theta (x|z)</script> network be a discriminator function D, who tries to make <script type="math/tex">D(G(z)) \rightarrow 0</script> and <script type="math/tex">D(x) \rightarrow 1</script> as close as possible, where x comes from real data and G(z) comes from the counterfeit. Let the <script type="math/tex">q_\phi (z|x)</script> network be a counterfeiter <script type="math/tex">G</script>, which strives to make it as hard as possible for the discriminator to correctly tell the difference. In addition, modify the training goal, so that instead of ELBO, optimization minimizes the “cross-entropy loss training a standard binary classifier with a sigmoid output”:</p>
<script type="math/tex; mode=display">J(\theta, \phi) = -\frac{1}{2} \mathbb{E}_{q_\phi} log D(\mathbf{x}) - \frac{1}{2} \mathbb{E}_{p_\theta} log (1-D(G(\mathbf{x})))</script><p>If we rename <script type="math/tex">q_\phi</script> to <script type="math/tex">p_g</script>, and <script type="math/tex">p_\theta</script> to <script type="math/tex">p_d</script>, the model in the GAN paper<sup id="fnref:4"><a href="#fn:4" rel="footnote">4</a></sup> is established.</p>
<p>This is not a small modification to the model. First, the GAN model minimizes JS divergence instead of KL. The f-GAN<sup id="fnref:5"><a href="#fn:5" rel="footnote">5</a></sup> paper generalized GAN model to any f-divergence families (<script type="math/tex">D_f(P||Q) = \int_\chi q(x) f(\frac{p(x)}{q(x)}) dx</script> where <script type="math/tex">f:\mathbb{R}_+ \rightarrow \mathbb{R}</script> is a convex generator function). Second, the decoder / discriminator input come from different places. VAE adds a noise $p(\epsilon)$ to an approximated encoder $q_\phi (z|x)$, and feed the decoder with a reparameterized sample $\tilde{z}$. GAN directly samples from the generator labeled as counterfeit data, samples from some prior labeled as ground-truth, and feed both into the discriminator.  </p>
<p>An important idea that GAN brings in is adversarial training. After the GAN paper came out, there have been a bunch of works incorporating the adversarial training framework, including:</p>
<ul>
<li>DCGAN<sup id="fnref:6"><a href="#fn:6" rel="footnote">6</a></sup> applying the GAN to deep convolutional nets.  </li>
<li>WGAN<sup id="fnref:7"><a href="#fn:7" rel="footnote">7</a></sup> minimizing Wasserstein distance, a weaker one in comparison to KL or JS.  </li>
<li>Many others.  </li>
</ul>
<p>References:</p>
<div id="footnotes"><hr><div id="footnotelist"><ol style="list-style:none; padding-left: 0;"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">1.</span><span style="display: inline-block; vertical-align: top;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1312.6114">Auto-encoding Variational Bayes</a></span><a href="#fnref:1" rev="footnote"> ↩</a></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">2.</span><span style="display: inline-block; vertical-align: top;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1206.7051">Stochastic Variational Inference</a></span><a href="#fnref:2" rev="footnote"> ↩</a></li><li id="fn:3"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">3.</span><span style="display: inline-block; vertical-align: top;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1603.00788">Automatic Differentiation Variational Inference</a></span><a href="#fnref:3" rev="footnote"> ↩</a></li><li id="fn:4"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">4.</span><span style="display: inline-block; vertical-align: top;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1406.2661">Generative Adversal Nets</a></span><a href="#fnref:4" rev="footnote"> ↩</a></li><li id="fn:5"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">5.</span><span style="display: inline-block; vertical-align: top;"><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.00709">f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization</a></span><a href="#fnref:5" rev="footnote"> ↩</a></li><li id="fn:6"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">6.</span><span style="display: inline-block; vertical-align: top;"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1511.06434v2.pdf">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a></span><a href="#fnref:6" rev="footnote"> ↩</a></li><li id="fn:7"><span style="display: inline-block; vertical-align: top; padding-right: 10px;">7.</span><span style="display: inline-block; vertical-align: top;"><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1701.07875.pdf">Wasserstein GAN</a></span><a href="#fnref:7" rev="footnote"> ↩</a></li></ol></div></div>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://ziningzhu.github.io/Blog/2017/05/14/2017_05_VAE_GAN/" data-id="clm25s9y8000uryrq0lmyfl17" class="article-share-link">Share</a>
      
        <a href="https://ziningzhu.github.io/Blog/2017/05/14/2017_05_VAE_GAN/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/GAN/" rel="tag">GAN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/VAE/" rel="tag">VAE</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/05/27/2017_05_intro_to_phil/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Intro to Philosophy
        
      </div>
    </a>
  
  
    <a href="/2017/05/03/2017_05_OS/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Systems Software</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>
</section>
        
      </div>
      <footer id="footer">
  
    <aside id="sidebar" class="outer">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/07/14/2023_07_state_vector/">&bull; A state-vector framework for dataset effects</a>
          </li>
        
          <li>
            <a href="/2023/06/26/2023_06_situated_NLE/">&bull; Situated Natural Language Explanation</a>
          </li>
        
          <li>
            <a href="/2022/12/31/2022_12_books/">&bull; Interesting Books in 2022</a>
          </li>
        
          <li>
            <a href="/2022/11/12/2022_11_fine_tuning_with_probing/">&bull; Predicting Fine-tuning Performance with Probing</a>
          </li>
        
          <li>
            <a href="/2022/07/31/2022_07_ood_probe/">&bull; OOD-Probe: A Neural Interpretation of Out-of-Domain Generalization</a>
          </li>
        
          <li>
            <a href="/2022/07/30/2022_07_ood_labels/">&bull; Out-of-Distribution Failure through the Lens of Labeling Mechanisms: An Information Theoretic Approach</a>
          </li>
        
          <li>
            <a href="/2022/04/27/2022_05_voiceprivacy/">&bull; Voice Anonymization by Multimodal Adaptive Noise</a>
          </li>
        
          <li>
            <a href="/2022/04/23/2022_04_probing_datasets/">&bull; On the data requirements of probing</a>
          </li>
        
          <li>
            <a href="/2022/04/23/2022_04_cxg_probing/">&bull; Neural reality of argument structure constructions</a>
          </li>
        
          <li>
            <a href="/2021/12/28/2021_12_books/">&bull; Cool Books in 2021</a>
          </li>
        
          <li>
            <a href="/2021/11/07/2021_11_task_specific_information/">&bull; Quantifying the Task-Specific Information in Text-Based Classifications</a>
          </li>
        
          <li>
            <a href="/2021/11/07/2021_11_detect_moral_sentiment_change/">&bull; An unsupervised framework for tracing textual sources of moral change</a>
          </li>
        
          <li>
            <a href="/2021/07/20/2021_07_writing_features/">&bull; What do writing features tell us about AI papers?</a>
          </li>
        
          <li>
            <a href="/2021/07/10/2021_07_layerwise_anomaly/">&bull; How is BERT surprised? Layerwise detection of linguistic anomalies</a>
          </li>
        
          <li>
            <a href="/2021/06/10/2021_06_NAACL/">&bull; NAACL 2021 papers</a>
          </li>
        
      </ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/ExplainableAI/" style="font-size: 18.75px;">ExplainableAI</a> <a href="/tags/GAN/" style="font-size: 10px;">GAN</a> <a href="/tags/Information/" style="font-size: 10px;">Information</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/OOD/" style="font-size: 11.25px;">OOD</a> <a href="/tags/PGM/" style="font-size: 10px;">PGM</a> <a href="/tags/Probing/" style="font-size: 10px;">Probing</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Society/" style="font-size: 10px;">Society</a> <a href="/tags/Speech/" style="font-size: 10px;">Speech</a> <a href="/tags/VAE/" style="font-size: 10px;">VAE</a> <a href="/tags/algorithms/" style="font-size: 11.25px;">algorithms</a> <a href="/tags/android/" style="font-size: 10px;">android</a> <a href="/tags/bash/" style="font-size: 10px;">bash</a> <a href="/tags/book-review/" style="font-size: 16.25px;">book-review</a> <a href="/tags/conferences/" style="font-size: 15px;">conferences</a> <a href="/tags/database/" style="font-size: 10px;">database</a> <a href="/tags/drone/" style="font-size: 10px;">drone</a> <a href="/tags/java/" style="font-size: 11.25px;">java</a> <a href="/tags/js/" style="font-size: 10px;">js</a> <a href="/tags/learning/" style="font-size: 11.25px;">learning</a> <a href="/tags/linguistics/" style="font-size: 12.5px;">linguistics</a> <a href="/tags/ml-basics/" style="font-size: 17.5px;">ml-basics</a> <a href="/tags/os/" style="font-size: 10px;">os</a> <a href="/tags/philosophy/" style="font-size: 13.75px;">philosophy</a> <a href="/tags/pytorch/" style="font-size: 11.25px;">pytorch</a> <a href="/tags/robotics/" style="font-size: 10px;">robotics</a> <a href="/tags/teamwork/" style="font-size: 10px;">teamwork</a> <a href="/tags/web/" style="font-size: 13.75px;">web</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Academia/">Academia</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Literature/">Literature</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Philosophy/">Philosophy</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Software/">Software</a></li></ul>
    </div>
  </div>


  
</aside>
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 Zining Zhu
      <br>
      Powered by <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a> with <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo-theme-landscape">Landscape</a>.
    </div>
  </div>

</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~zining" class="mobile-nav-link">About</a>
  
</nav>
    
<script>
  var disqus_shortname = 'ziningzhu-me';
  
  var disqus_url = 'https://ziningzhu.github.io/Blog/2017/05/14/2017_05_VAE_GAN/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" id=""></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>