<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Probability | Explainable and Controllable AI Lab</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="This blog is a review of probability information.  Probability Distributions Conditional Probability and Bayes Theorem Estimation: ML, MAP From condition probability to Bayesian Networks   Example: Na">
<meta property="og:type" content="article">
<meta property="og:title" content="Probability">
<meta property="og:url" content="https://ziningzhu.github.io/2017/02/21/2017_02_probability/index.html">
<meta property="og:site_name" content="Explainable and Controllable AI Lab">
<meta property="og:description" content="This blog is a review of probability information.  Probability Distributions Conditional Probability and Bayes Theorem Estimation: ML, MAP From condition probability to Bayesian Networks   Example: Na">
<meta property="og:locale">
<meta property="og:image" content="https://ziningzhu.github.io/img/Naive_Bayes_Classifier.png">
<meta property="og:image" content="https://ziningzhu.github.io/img/Bayesian_Tracking.png">
<meta property="og:image" content="https://ziningzhu.github.io/img/Bayesian_Tracking_Plate_Model.png">
<meta property="article:published_time" content="2017-02-22T02:23:00.000Z">
<meta property="article:modified_time" content="2023-09-02T15:05:29.196Z">
<meta property="article:author" content="Zining Zhu">
<meta property="article:tag" content="ml-basics">
<meta property="article:tag" content="PGM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ziningzhu.github.io/img/Naive_Bayes_Classifier.png">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-84222809-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  <meta name="google-site-verification" content="DI7qg6_y3mwWTdMA-GjrIY60uIS0beb12GxXZZ4A6gw" />

  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <script src="https://kit.fontawesome.com/07c95db82c.js"></script>
<meta name="generator" content="Hexo 5.4.2"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Explainable and Controllable AI Lab</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/research">Research</a>
        
          <a class="main-nav-link" href="/team">Team</a>
        
      </nav>
      <nav id="sub-nav">
        
        
        
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://ziningzhu.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-2017_02_probability" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/02/21/2017_02_probability/" class="article-date">
  <time datetime="2017-02-22T02:23:00.000Z" itemprop="datePublished">2017-02-21</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Probability
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>This blog is a review of probability information.</p>
<ul>
<li>Probability Distributions</li>
<li>Conditional Probability and Bayes Theorem</li>
<li>Estimation: ML, MAP</li>
<li>From condition probability to Bayesian Networks  </li>
<li>Example: Naive Bayes Model</li>
<li>Example: Bayesian Tracking, Temporal Model and the Plate Model  </li>
<li>From Directed Model to Undirected Model  </li>
<li>Exact Inference</li>
<li>Message Passing and Belief Propagation  </li>
</ul>
<span id="more"></span>
<h3 id="Probability-Distributions"><a href="#Probability-Distributions" class="headerlink" title="Probability Distributions"></a>Probability Distributions</h3><ul>
<li>For a variable x, if it satisfies a distribution p(x), we can write as $x\sim p(x)$. The probability that x appears as a particular value $x_0$ is $p(x_0)$.  </li>
<li>A joint probability distribution for several variables is $p(x_1, x_2, …)$</li>
<li>Several conditions of probability distributions:<ul>
<li>Marginalization: $\int p(x) dx = 1$ (if x is continuous) and $\sum p(x) = 1$ (if x is discrete). The total probability is 1.</li>
<li>Marginalization for joint probability distributions: $\int p(x,y) dy = p(x)$</li>
<li>Factorization: $p(x,y)=p(x)p(y)$ if and only if x and y are independent</li>
</ul>
</li>
<li>Many people write probabiltiy distibutions as functions. $f_{x}(x_0)$, $p(x_0)$, and $p(x=x_0)$ are the same thing.</li>
</ul>
<h3 id="Conditional-Probability-and-Bayes-Theorem"><a href="#Conditional-Probability-and-Bayes-Theorem" class="headerlink" title="Conditional Probability and Bayes Theorem"></a>Conditional Probability and Bayes Theorem</h3><ul>
<li>The probability of x given y: $p(x|y)=\frac{p(x,y)}{p(y)}$</li>
<li>Bayes Theorem: $p(x|y)=\frac{p(y|x)p(x)}{p(y)}$</li>
<li>If we interpret x as the variable, y as the observation, then $p(x)$ is “prior belief”, $p(y|x)$ is the likelihood, $p(y)=\sum_x p(y|x)p(x)$ is the nomalization constant, and $p(x|y)$ is the “a posterior” (“afterwards”) distribution. This idea is generalized in the Bayesian tracking, where the a posterior belief is updated every time when a new observation comes in.</li>
</ul>
<h3 id="Estimation-MLE-MAP"><a href="#Estimation-MLE-MAP" class="headerlink" title="Estimation: MLE, MAP"></a>Estimation: MLE, MAP</h3><ul>
<li>Now that we have the probability distribution $p(\mathbf{x};\theta)$ where x is data and $\theta$ is (unknown) parameter.  </li>
<li>Define likelihood (function of $\theta$): $L(\theta; \mathbf{x})=p(\mathbf{x}|\theta)$ as the probability of getting such data given the $\theta$.  </li>
<li>Maximizing the likelihood function results in the Maximum Likelihood Estimation (MLE): $\theta^{MLE}=argmax_{\theta}p(x|\theta)$.  </li>
<li>Define the <strong>a posteriori probability</strong> as $p(\theta | x)$, which is the probability of $\theta$ given data x.</li>
<li>Then the “maximum a posteriori probability” $\theta^{MAP}=argmax_{\theta} p(\theta|x)$ is the $\theta$ that results in a maximum a posteriori probability.  </li>
<li>Note that MAP can be calculated using MLE.   <script type="math/tex">\theta^{MAP}=argmax_{\theta} p(\theta|x)=argmax_{\theta} \frac{p(x|\theta)p(\theta)}{p(x)}</script><script type="math/tex; mode=display">=argmax_{\theta} \frac{p(x|\theta)p(\theta)}{\sum_{\theta} p(x|\theta)p(\theta)}=argmax_{\theta}p(x|\theta)g(\theta)</script></li>
<li>So when $g(\theta)$ is constant, MAP is the same as MLE.</li>
</ul>
<h3 id="From-Condition-Probability-to-Bayesian-Network"><a href="#From-Condition-Probability-to-Bayesian-Network" class="headerlink" title="From Condition Probability to Bayesian Network"></a>From Condition Probability to Bayesian Network</h3><ul>
<li>We have seen that random variables can interact with each other, as described by the dependencies.  </li>
<li>Using graphical model to represent the dependencies between variables is an intuitive way.</li>
<li>Use nodes to represent random variables, and edges to represent dependency. $x \rightarrow y$ means the random variable x depends on y.</li>
<li>To decide a joint distribution, we need a condition probability distribution for each node. For example, the joint probability distribution corresponding to the graph $x \rightarrow y$ can be written as  <script type="math/tex; mode=display">f_{x,y}(x,y)=f_y(y)f_{x|y}(x|y)</script></li>
<li>Such type of graph is called <strong>Bayesian Network</strong> (BN).</li>
<li>Bayesian Network is one of the two most common Probabilistic Graphics Models (PGM). The other type of PGM is <strong>Markov Random Fields</strong> (MRF), where the graphs are undirected.</li>
<li>The “model” in PGM refers to the machine learning models.</li>
<li>There are at least two obvious benefits of using PGM. First, it provides a clear visualization of conditional dependence of variables. Dependencies, or inferences, of variables can flow within a Bayesian Network according to some rules (Bayes Ball or d-separation). Second, it provides a rich language and framework to describe machine learning algorithms.</li>
<li>Two variables are said to be <strong>d-separated</strong> if dependency cannot flow from one to the other. There are four types of junctions:<br>(1) $A\rightarrow S \rightarrow B$. When S is unobserved, dependency can flow from A to B. When S is observed, it can not.<br>(2) $A \leftarrow S \leftarrow B$. Same as (1).<br>(3) $A \leftarrow S \rightarrow B$. Same as (1).<br>(4) $A \rightarrow S \leftarrow B$. “V-structure”. When S or any of successors of is observed, dependency can flow from A to B. It cannot otherwise. Why is this case different? Consider the example where A is “cleverness”, S is “score”, and B is “exam average”. Given the score, the lower the exam average is, the more clever the student is.  </li>
<li>If dependency can flow through, then the path is an <strong>active trail</strong>.</li>
</ul>
<h3 id="Example-Naive-Bayes-Model"><a href="#Example-Naive-Bayes-Model" class="headerlink" title="Example: Naive Bayes Model"></a>Example: Naive Bayes Model</h3><ul>
<li>An example of a model using Bayesian Network representation is NBM.</li>
<li><img src="/img/Naive_Bayes_Classifier.png" alt="figure taken from Koller&#39;s PGM slides">  </li>
<li>The overall idea of NBM is, behind the data (represented by $x$), there is a variable denoting the “Class” (represented by $C$) upon which all variables depend. Once the class is determined, the variables are independent of each other. The joint probability distribution is<script type="math/tex; mode=display">p(C;x_1,x_2,...,x_n)=p(C)\Pi_{i=1}^n p(x_i|C)</script></li>
<li>The Naive Bayes Model can be used as classifier (Naive Bayes Classifier). The algorithm is to find the MAP estimation of class: the class that maximizes $P(C|x_1, x_2, …)$.</li>
<li>Practical note: In implementation, NBM uses a “add-one” rule, in order to avoid each of the probabilities to fall down to zero. Why? Usually when there is a zero probability, the reason is not that this class shall not contain this data, but because your training data is not large enough to include the data in this class.</li>
</ul>
<h3 id="Example-Bayesian-Tracking-Temporal-Model-and-Plate-Model"><a href="#Example-Bayesian-Tracking-Temporal-Model-and-Plate-Model" class="headerlink" title="Example: Bayesian Tracking, Temporal Model and Plate Model"></a>Example: Bayesian Tracking, Temporal Model and Plate Model</h3><ul>
<li>Another example of Bayesian estimation is Bayesian Tracking. The underlying idea is to update a posteriori estimation after each observation.  </li>
<li>Let us first look at the system. It can be represented by the following Bayesian Network: <img src="/img/Bayesian_Tracking.png" alt="Bayesian Tracking, from Bleser and Stricker&#39;s slides"></li>
<li>where x is the state variable, y is the observation, u is control signal, v and w are two sources of random noises. z is observable whereas x and u are not. With the knowledge about the system state transition (a.k.a: <script type="math/tex">p(x_t|x_{t-1})</script>) and the measurement model (a.k.a: <script type="math/tex">p(z_t|x_t)</script>), you want to give an estimation about the state variable x at each time step.</li>
<li>The algorithm is as following.<br>First give a prior estimation (guess) about x: $p(x_0)$.<br>Then for each time step:<br>  Calculate the a priori estimation:  <script type="math/tex; mode=display">p(x_t|z_{1:t-1}) = \sum_{x_t} p(x_t|x_{t-1}) p(x_t|z_{1:t-1})</script>  Calculate the a posteriori estimation:  <script type="math/tex; mode=display">p(x_t|z_{1:t}) = p(x_t|z_t,z_{1:t-1}) = \frac{f(z_k|x_k,z_{1:k-1}) f(x_k|z_{1:k-1})}{f(z_k|z_{1:k-1})} = \frac{f(z_k|x_k) f(x_k|z_{1:k-1})}{Const}</script></li>
<li>In the last line, the <script type="math/tex">z_{1:k-1}</script> term is dropped.<br>Why? Because <script type="math/tex">z_k</script> does not depend on <script type="math/tex">z_{1:k-1}</script> since the previous state is given. Similarly, the denominator is just a normalization constant.</li>
<li>The graphical model above is an example of <strong>temporal model</strong>, where many repetitive steps occur together. Hence a <strong>plate model</strong> can be used to simplify the graph. <img src="/img/Bayesian_Tracking_Plate_Model.png" alt="Bayesian Tracking Plate Model"></li>
<li>Sidenote: the PGM for Bayesian tracking without random noise is actually a Hidden Markov Model (HMM).</li>
</ul>
<h3 id="From-Directed-to-Undirected-Model"><a href="#From-Directed-to-Undirected-Model" class="headerlink" title="From Directed to Undirected Model"></a>From Directed to Undirected Model</h3><ul>
<li>Above mentioned are Bayesian Network. The other most common type of PGM, Markov Random Field, is represented by undirected graph models.</li>
<li>Vertices still represent random variables, and edge represent the inference between the variables.</li>
<li>We talk more about “infernce” instead of “dependency” in undirected models.  </li>
<li>The probability here is instead written as factor. For example, the MRF $x - y - z$ corresponds to the factor $\psi(x,y)\psi(y,z)$.</li>
<li>Converting a directed model to an undirected graphical models loses some information. Also, <strong>moralization</strong> should be performed. The moralization process is needed when two or more nodes are parents of a child node. In order to keep the inference flowing, in MRF all parents should be connected (into a clique).  </li>
<li>An example shows the necessity of moralization. In a v-structured model ($x\rightarrow w \leftarrow y$), the probability is $p(x)p(y)p(w|x,y)$. However, without moralization, the factor representation of $x-w-y$ is $\psi(x,w)\psi(w,y)$. There should be a term considering w,x,y simultanuously but there is not! So x and y should be “moralized”.</li>
<li>A <strong>clique</strong> in a MRF is a subgraph where every single nodes are connected to everyone else.</li>
</ul>
<h3 id="Exact-Inference"><a href="#Exact-Inference" class="headerlink" title="Exact Inference"></a>Exact Inference</h3><ul>
<li>Problem: given a joint probability distribution <script type="math/tex">p(X_1, X_2, ...)</script> (where <script type="math/tex">x_i, x_{i+1}...</script> are unobserved), evaluate the marginaliation over unobserved variables.</li>
<li>Naively marginalize over all unobserved variables require an exponential number of computations.</li>
<li>Use Variable Elimination to compute marginalization. Push some variables inside.</li>
<li>During elimination of each variable, an <strong>induced graph</strong> is produced.</li>
<li>The <strong>width</strong> of an induced graph is number of nodes in the largest clique minus 1.</li>
<li>Define the <strong>tree width</strong> (or <strong>minimal induced width</strong>) of a graph to be the minimum of the induced graph width (when eliminating according to a particular ordering).</li>
<li>Finding the best elimination ordering (also computing the tree width) is NP-hard.</li>
</ul>
<h3 id="Message-Passing-and-Belief-Propagation"><a href="#Message-Passing-and-Belief-Propagation" class="headerlink" title="Message Passing and Belief Propagation"></a>Message Passing and Belief Propagation</h3><ul>
<li>Some information from this section is taken from CSC412 slides at University of Toronto, <a target="_blank" rel="noopener" href="http://computerrobotvision.org/2009/tutorial_day/crv09_belief_propagation_v2.pdf">this tutorial</a> and <a target="_blank" rel="noopener" href="http://6.869.csail.mit.edu/fa13/lectures/slideNotesCh7rev.pdf">this note</a></li>
<li>A <strong>message</strong> is a re-usable partial sum for the marginalization calculations.</li>
<li>Message can be passeed from nodes to nodes. For example, the message from variable j to variable i (coming into the node $x_i$) is:<script type="math/tex; mode=display">m_{ji}(x_i)=\sum_{x_j}\psi_{ij}(x_i,x_j) \Pi_{k\in \eta (j) \backslash i} m_{kj}(x_j)</script>where $\eta(j)$ contains all neighbouring nodes of j.</li>
<li>The marginal probability at a node is the produce of all incoming messages at that node:<script type="math/tex; mode=display">p_i (x_i)=\Pi_{j\in \eta(i)} m_{ji}x_i</script></li>
<li>Sum-product Belief Propagation (root-to-leaf, and then leaf-to-root)</li>
<li>Max-product BP</li>
<li>Comparison of max product versus sum product.</li>
<li>Loopy BP algorithm</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://ziningzhu.github.io/2017/02/21/2017_02_probability/" data-id="cm0f5ld3f000z5nrq022t6f0z" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PGM/" rel="tag">PGM</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ml-basics/" rel="tag">ml-basics</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/05/27/2017_05_intro_to_phil/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Intro to Philosophy
        
      </div>
    </a>
  
  
    <a href="/2017/02/12/2017_02_ML_starter/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Machine Learning Starter Notes</div>
    </a>
  
</nav>

  
</article>

</section>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2024 Zining Zhu
      <br>
      Powered by <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a> with <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo-theme-landscape">Landscape</a>.
    </div>
  </div>

</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/research" class="mobile-nav-link">Research</a>
  
    <a href="/team" class="mobile-nav-link">Team</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" id=""></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>