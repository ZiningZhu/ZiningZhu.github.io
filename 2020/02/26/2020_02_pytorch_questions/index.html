<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Pytorch Questions | Explainable and Controllable AI Lab</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="This notebook compiles experiments to three questions wrt pytorch usage.    Memory overflow: what if you don’t GC the graph?   Gradient zeroing: when to do it, and why necessary?   Backward second tim">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch Questions">
<meta property="og:url" content="https://ziningzhu.github.io/2020/02/26/2020_02_pytorch_questions/index.html">
<meta property="og:site_name" content="Explainable and Controllable AI Lab">
<meta property="og:description" content="This notebook compiles experiments to three questions wrt pytorch usage.    Memory overflow: what if you don’t GC the graph?   Gradient zeroing: when to do it, and why necessary?   Backward second tim">
<meta property="og:locale">
<meta property="article:published_time" content="2020-02-26T19:27:00.000Z">
<meta property="article:modified_time" content="2023-09-02T15:05:29.202Z">
<meta property="article:author" content="Zining Zhu">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-84222809-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  <meta name="google-site-verification" content="DI7qg6_y3mwWTdMA-GjrIY60uIS0beb12GxXZZ4A6gw" />

  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <script src="https://use.fontawesome.com/93f0a51ca6.js"></script>
<meta name="generator" content="Hexo 5.4.2"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Explainable and Controllable AI Lab</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/research">Research</a>
        
          <a class="main-nav-link" href="/team">Team</a>
        
      </nav>
      <nav id="sub-nav">
        
        
        
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://ziningzhu.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-2020_02_pytorch_questions" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/02/26/2020_02_pytorch_questions/" class="article-date">
  <time datetime="2020-02-26T19:27:00.000Z" itemprop="datePublished">2020-02-26</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Software/">Software</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Pytorch Questions
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><a target="_blank" rel="noopener" href="https://res.cloudinary.com/dnijsrvoc/raw/upload/v1650725869/Blog/res/pytorch_questions_nue7fa.ipynb">This notebook</a> compiles experiments to three questions wrt pytorch usage.  </p>
<ol>
<li>Memory overflow: what if you don’t GC the graph?  </li>
<li>Gradient zeroing: when to do it, and why necessary?  </li>
<li>Backward second time: when will this be triggered?  </li>
</ol>
<span id="more"></span>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> psutil</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br></pre></td></tr></table></figure>
<h2 id="Memory-Overflow"><a href="#Memory-Overflow" class="headerlink" title="Memory Overflow"></a>Memory Overflow</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">print_memory_usage</span>(<span class="params">label</span>):</span><br><span class="line">    process = psutil.Process(os.getpid())</span><br><span class="line">    <span class="comment">#print(process.memory_info())</span></span><br><span class="line">    mem = process.memory_info().rss / <span class="number">1024</span> / <span class="number">1024</span></span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;&#123;&#125; using &#123;:.2f&#125; MB memory!&quot;</span>.<span class="built_in">format</span>(label, mem))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">memory_overflow</span>():</span><br><span class="line">    </span><br><span class="line">    network = nn.Sequential(</span><br><span class="line">        nn.Linear(<span class="number">4</span>, <span class="number">10000</span>), nn.Linear(<span class="number">10000</span>, <span class="number">10000</span>), nn.Linear(<span class="number">10000</span>, <span class="number">2</span>))</span><br><span class="line">    losses = []</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">101</span>):</span><br><span class="line">        x = torch.tensor([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>)  </span><br><span class="line">        y = network(x)</span><br><span class="line">        loss = y * y</span><br><span class="line">        losses.append(loss.mean())  <span class="comment"># A reference to the previous graphs are not freed.</span></span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            print_memory_usage(<span class="string">&quot;Epoch &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch))</span><br><span class="line">        </span><br><span class="line">gc.collect()</span><br><span class="line">memory_overflow()</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 10 using 513.77 MB memory!
Epoch 20 using 514.53 MB memory!
Epoch 30 using 515.36 MB memory!
Epoch 40 using 516.20 MB memory!
Epoch 50 using 517.04 MB memory!
Epoch 60 using 517.87 MB memory!
Epoch 70 using 518.71 MB memory!
Epoch 80 using 519.55 MB memory!
Epoch 90 using 520.39 MB memory!
Epoch 100 using 521.23 MB memory!
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">no_memory_overflow</span>():</span><br><span class="line">    </span><br><span class="line">    network = nn.Sequential(</span><br><span class="line">        nn.Linear(<span class="number">4</span>, <span class="number">10000</span>), nn.Linear(<span class="number">10000</span>, <span class="number">10000</span>), nn.Linear(<span class="number">10000</span>, <span class="number">2</span>))</span><br><span class="line">    losses = []</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">101</span>):</span><br><span class="line">        x = torch.tensor([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>)  </span><br><span class="line">        y = network(x)</span><br><span class="line">        loss = y * y</span><br><span class="line">        losses.append(loss.mean().item())  <span class="comment"># The previous graph could be discarded</span></span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">            print_memory_usage(<span class="string">&quot;Epoch &#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch))</span><br><span class="line">        </span><br><span class="line">gc.collect()</span><br><span class="line">no_memory_overflow()</span><br></pre></td></tr></table></figure>
<pre><code>Epoch 10 using 513.01 MB memory!
Epoch 20 using 513.03 MB memory!
Epoch 30 using 513.03 MB memory!
Epoch 40 using 513.03 MB memory!
Epoch 50 using 513.03 MB memory!
Epoch 60 using 513.03 MB memory!
Epoch 70 using 513.04 MB memory!
Epoch 80 using 513.04 MB memory!
Epoch 90 using 513.04 MB memory!
Epoch 100 using 513.04 MB memory!
</code></pre><h2 id="Gradient-Zeroing"><a href="#Gradient-Zeroing" class="headerlink" title="Gradient Zeroing"></a>Gradient Zeroing</h2><p><a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch/4903">This discussion</a> could be relevant.<br>If you don’t zero out the gradient, you effectively increased the batch size. This could be useful when you want to train very large batch of data. An example is in <a target="_blank" rel="noopener" href="https://github.com/OpenNMT/OpenNMT-py">Open NMT</a> (in its —accum_count option).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test_gradient_accumulation</span>():</span><br><span class="line">    x = torch.ones((<span class="number">1</span>), requires_grad=<span class="literal">False</span>)  <span class="comment"># requires_grad is default to False</span></span><br><span class="line">    w = torch.ones((<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    y = w * x</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;Before backward(): w.grad_=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(w.grad))</span><br><span class="line">    y.backward()</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;After backward():  w.grad_=&#123;&#125; (should be 1)&quot;</span>.<span class="built_in">format</span>(w.grad))</span><br><span class="line">    </span><br><span class="line">    y = w * x</span><br><span class="line">    y.backward()</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;Second pass:       w.grad=&#123;&#125; (accumulated to 2)&quot;</span>.<span class="built_in">format</span>(w.grad))</span><br><span class="line">    </span><br><span class="line">    w.grad.data.zero_()</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;===I zero&#x27;ed the grad===&quot;</span>)</span><br><span class="line">    y = w * x</span><br><span class="line">    y.backward()</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;Now:               w.grad=&#123;&#125; (back to 1, the correct value)&quot;</span>.<span class="built_in">format</span>(w.grad))</span><br><span class="line">    </span><br><span class="line">test_gradient_accumulation()</span><br></pre></td></tr></table></figure>
<pre><code>Before backward(): w.grad_=None
After backward():  w.grad_=tensor([1.]) (should be 1)
Second pass:       w.grad=tensor([2.]) (accumulated to 2)
===I zero&#39;ed the grad===
Now:               w.grad=tensor([1.]) (back to 1, the correct value)
</code></pre><h2 id="Backward-second-time"><a href="#Backward-second-time" class="headerlink" title="Backward second time"></a>Backward second time</h2><p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/46774641/what-does-the-parameter-retain-graph-mean-in-the-variables-backward-method">This stackoverflow question</a> is relevant.<br>See <a target="_blank" rel="noopener" href="https://discuss.pytorch.org/t/which-is-freed-which-is-not/8636">this discussion</a> for when a node is freed and when it is not.  </p>
<p>Basically, this depends on your node types:</p>
<ul>
<li>The <strong>root node</strong> (<code>loss</code>) does not have gradient (dL/dL is not useful). </li>
<li>The <strong>leaves nodes</strong> (<code>weights</code>) have their gradients accumulated if you do the second backprop.</li>
<li>Your <strong>intermediate nodes</strong> (all other nodes) are free’d during <code>backward()</code> unless you <code>backward(retain_graph=True)</code>. This causes the “Backward Second Time” error.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test_backprop_second_time</span>():</span><br><span class="line">    x = torch.ones((<span class="number">1</span>), requires_grad=<span class="literal">False</span>)</span><br><span class="line">    w = torch.ones((<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    y = w * x  <span class="comment"># y is not a leaf node</span></span><br><span class="line">    loss = y * y  </span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;After backward():  x=&#123;&#125;, x.grad=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(x, x.grad))</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;After backward():  w=&#123;&#125;, w.grad=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(w, w.grad))</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;After backward():  y=&#123;&#125;, y.grad=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(y, y.grad))</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;After backward():  loss=&#123;&#125;, loss.grad=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(loss, loss.grad))</span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">test_backprop_second_time()</span><br></pre></td></tr></table></figure>
<pre><code>After backward():  x=tensor([1.]), x.grad=None
After backward():  w=tensor([1.], requires_grad=True), w.grad=tensor([2.])
After backward():  y=tensor([1.], grad_fn=&lt;MulBackward0&gt;), y.grad=None
After backward():  loss=tensor([1.], grad_fn=&lt;MulBackward0&gt;), loss.grad=None



---------------------------------------------------------------------------

RuntimeError                              Traceback (most recent call last)

&lt;ipython-input-47-6b7e5cd304a0&gt; in &lt;module&gt;
     11     loss.backward()
     12 
---&gt; 13 test_backprop_second_time()


&lt;ipython-input-47-6b7e5cd304a0&gt; in test_backprop_second_time()
      9     print (&quot;After backward():  y=&#123;&#125;, y.grad=&#123;&#125;&quot;.format(y, y.grad))
     10     print (&quot;After backward():  loss=&#123;&#125;, loss.grad=&#123;&#125;&quot;.format(loss, loss.grad))
---&gt; 11     loss.backward()
     12 
     13 test_backprop_second_time()


~/anaconda3/envs/pytorch12/lib/python3.8/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)
    193                 products. Defaults to ``False``.
    194         &quot;&quot;&quot;
--&gt; 195         torch.autograd.backward(self, gradient, retain_graph, create_graph)
    196 
    197     def register_hook(self, hook):


~/anaconda3/envs/pytorch12/lib/python3.8/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)
     95         retain_graph = create_graph
     96 
---&gt; 97     Variable._execution_engine.run_backward(
     98         tensors, grad_tensors, retain_graph, create_graph,
     99         allow_unreachable=True)  # allow_unreachable flag


RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">test_backprop_second_time</span>():</span><br><span class="line">    x = torch.ones((<span class="number">1</span>), requires_grad=<span class="literal">False</span>)</span><br><span class="line">    w = torch.ones((<span class="number">1</span>), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    y = w * x  <span class="comment"># y is not a leaf node</span></span><br><span class="line">    loss = y * y  </span><br><span class="line">    loss.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;=== backward() with retain_graph ===&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;After backward():  x=&#123;&#125;, x.grad=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(x, x.grad))</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;After backward():  w=&#123;&#125;, w.grad=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(w, w.grad))</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;After backward():  y=&#123;&#125;, y.grad=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(y, y.grad))</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;After backward():  loss=&#123;&#125;, loss.grad=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(loss, loss.grad))</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;=== backward() again ===&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;After backward():  x=&#123;&#125;, x.grad=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(x, x.grad))</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;After backward():  w=&#123;&#125;, w.grad=&#123;&#125; (you can see gradient accumulation too)&quot;</span>.<span class="built_in">format</span>(w, w.grad))</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;After backward():  y=&#123;&#125;, y.grad=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(y, y.grad))</span><br><span class="line">    <span class="built_in">print</span> (<span class="string">&quot;After backward():  loss=&#123;&#125;, loss.grad=&#123;&#125;&quot;</span>.<span class="built_in">format</span>(loss, loss.grad))</span><br><span class="line">    </span><br><span class="line">test_backprop_second_time()</span><br></pre></td></tr></table></figure>
<pre><code>=== backward() with retain_graph ===
After backward():  x=tensor([1.]), x.grad=None
After backward():  w=tensor([1.], requires_grad=True), w.grad=tensor([2.])
After backward():  y=tensor([1.], grad_fn=&lt;MulBackward0&gt;), y.grad=None
After backward():  loss=tensor([1.], grad_fn=&lt;MulBackward0&gt;), loss.grad=None
=== backward() again ===
After backward():  x=tensor([1.]), x.grad=None
After backward():  w=tensor([1.], requires_grad=True), w.grad=tensor([4.]) (you can see gradient accumulation too)
After backward():  y=tensor([1.], grad_fn=&lt;MulBackward0&gt;), y.grad=None
After backward():  loss=tensor([1.], grad_fn=&lt;MulBackward0&gt;), loss.grad=None
</code></pre>
      
    </div>
    <footer class="article-footer">
      <a data-url="https://ziningzhu.github.io/2020/02/26/2020_02_pytorch_questions/" data-id="cm0f5ld3k00285nrq2kpadf6g" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/pytorch/" rel="tag">pytorch</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2020/04/12/2020_04_bayes_recap/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Tested Positive. Do I Have It?
        
      </div>
    </a>
  
  
    <a href="/2020/02/12/2020_02_AAAI/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">AAAI2020</div>
    </a>
  
</nav>

  
</article>

</section>
        
      </div>
      <footer id="footer">
  
    <aside id="sidebar" class="outer">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/ExplainableAI/" style="font-size: 18.57px;">ExplainableAI</a> <a href="/tags/Information/" style="font-size: 10px;">Information</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/OOD/" style="font-size: 11.43px;">OOD</a> <a href="/tags/PGM/" style="font-size: 10px;">PGM</a> <a href="/tags/Probing/" style="font-size: 10px;">Probing</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Society/" style="font-size: 10px;">Society</a> <a href="/tags/Speech/" style="font-size: 10px;">Speech</a> <a href="/tags/algorithms/" style="font-size: 11.43px;">algorithms</a> <a href="/tags/android/" style="font-size: 10px;">android</a> <a href="/tags/bash/" style="font-size: 10px;">bash</a> <a href="/tags/book-review/" style="font-size: 10px;">book-review</a> <a href="/tags/conferences/" style="font-size: 15.71px;">conferences</a> <a href="/tags/database/" style="font-size: 10px;">database</a> <a href="/tags/drone/" style="font-size: 10px;">drone</a> <a href="/tags/java/" style="font-size: 11.43px;">java</a> <a href="/tags/js/" style="font-size: 10px;">js</a> <a href="/tags/learning/" style="font-size: 10px;">learning</a> <a href="/tags/linguistics/" style="font-size: 12.86px;">linguistics</a> <a href="/tags/ml-basics/" style="font-size: 17.14px;">ml-basics</a> <a href="/tags/philosophy/" style="font-size: 12.86px;">philosophy</a> <a href="/tags/pytorch/" style="font-size: 11.43px;">pytorch</a> <a href="/tags/robotics/" style="font-size: 10px;">robotics</a> <a href="/tags/web/" style="font-size: 14.29px;">web</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/">2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/">2022</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/">2021</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/">2020</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/">2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/">2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/">2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/">2015</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


  
</aside>
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2024 Zining Zhu
      <br>
      Powered by <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a> with <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo-theme-landscape">Landscape</a>.
    </div>
  </div>

</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/research" class="mobile-nav-link">Research</a>
  
    <a href="/team" class="mobile-nav-link">Team</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" id=""></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>