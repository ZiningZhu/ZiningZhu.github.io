<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>ML / NLP Interviews | Explainable and Controllable AI Lab</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="This blog compiles some points that might be encountered in ML &#x2F; NLP interviews. Good luck (for myself and to all readers finding jobs).    Statistics   Probability   Causality, Fair ML   Models   Neu">
<meta property="og:type" content="article">
<meta property="og:title" content="ML &#x2F; NLP Interviews">
<meta property="og:url" content="https://ziningzhu.github.io/2018/11/05/2018_11_ML_NLP_interview_prep/index.html">
<meta property="og:site_name" content="Explainable and Controllable AI Lab">
<meta property="og:description" content="This blog compiles some points that might be encountered in ML &#x2F; NLP interviews. Good luck (for myself and to all readers finding jobs).    Statistics   Probability   Causality, Fair ML   Models   Neu">
<meta property="og:locale">
<meta property="article:published_time" content="2018-11-05T05:12:00.000Z">
<meta property="article:modified_time" content="2023-09-02T15:05:29.199Z">
<meta property="article:author" content="Zining Zhu">
<meta property="article:tag" content="algorithms">
<meta property="article:tag" content="ml-basics">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-84222809-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  <meta name="google-site-verification" content="DI7qg6_y3mwWTdMA-GjrIY60uIS0beb12GxXZZ4A6gw" />

  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <script src="https://use.fontawesome.com/93f0a51ca6.js"></script>
<meta name="generator" content="Hexo 5.4.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Explainable and Controllable AI Lab</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/research">Research</a>
        
          <a class="main-nav-link" href="/teaching">Teaching</a>
        
          <a class="main-nav-link" href="/team">Team</a>
        
      </nav>
      <nav id="sub-nav">
        
        
        
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://ziningzhu.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-2018_11_ML_NLP_interview_prep" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/05/2018_11_ML_NLP_interview_prep/" class="article-date">
  <time datetime="2018-11-05T05:12:00.000Z" itemprop="datePublished">2018-11-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      ML / NLP Interviews
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>This blog compiles some points that might be encountered in ML / NLP interviews. Good luck (for myself and to all readers finding jobs).  </p>
<ul>
<li>Statistics  </li>
<li>Probability  </li>
<li>Causality, Fair ML  </li>
<li>Models  </li>
<li>Neural Network  </li>
<li>Training  </li>
<li>Natural Languages Processing specific  <ul>
<li>NLP Basics  </li>
<li>Information theory  </li>
<li>Embedding  </li>
<li>Parsing  </li>
</ul>
</li>
</ul>
<span id="more"></span>
<h2 id="Statistics"><a href="#Statistics" class="headerlink" title="Statistics"></a>Statistics</h2><ul>
<li>Evaluating a model. TP / FP / TN / FN  </li>
<li>Type 1 (false positive) and type 2 errors (false negative).  </li>
<li>Precision (TP / (TP + FP)) vs. Recall (TP / (TP + FN)).  </li>
<li>Sensitivity / specificity.  </li>
<li>F1 score definitions. Micro / macro / weighted.  </li>
<li>ROC: receiver operating characteristic) curve. True positive rate vs. false positive rate at various threshold settings.</li>
<li>AUC: area under (the ROC) curve.  </li>
<li>Bias-variance tradeoff. How to decide whether the model is overfitting, looking from learning plot?  <ul>
<li>High bias: overfit. Solve with more training.  </li>
<li>High variance: too flexible; low generalizability. Solve with (1) regularization (2) bagging algorithm (3) using less and more simple features.</li>
</ul>
</li>
<li>LOOCV, k-fold. They are designed to counteract the bias!   </li>
</ul>
<h2 id="Probability"><a href="#Probability" class="headerlink" title="Probability"></a>Probability</h2><ul>
<li>Bayes Rule: posterior = likelihood * prior / evidence.  </li>
<li>How to compute evidence? Sum them up / integral / etc.  </li>
<li>How to compute posterior when it is almost impossible to do the above steps?  Variational inference (to maximize the variational lower bound) or Monte Carlo sampling (e.g., Metropolis-Hastings algorithm, Gibbs sampling).  </li>
<li>Probability distribution functions (pdf).  </li>
<li>Exponential family distribution functions. Conjugate priors (Beta prior + binomial distribution; Dirichlet prior + multinomial distribution)  </li>
<li>Mutual information <script type="math/tex">I(X, Y) = \sum_X \sum_Y P(X, Y) log\frac{P(X, Y)}{P(X)P(Y)}</script>  </li>
<li>Conditional independence $P(X|Y) = P(Y)$  </li>
<li>Bayesian Network, d-separation.  </li>
<li>Variable elimination.  </li>
</ul>
<h2 id="Causality-and-Fair-Learning"><a href="#Causality-and-Fair-Learning" class="headerlink" title="Causality and Fair Learning"></a>Causality and Fair Learning</h2><ul>
<li>Randomized control trials.  </li>
<li>Rubin-Neyman causal model.  </li>
<li>Traditional deconfounding approaches: residualization, inverse probability weighting.  </li>
<li>Fair Learning: unfairness metrics (demographics parity, equalized opportunity)  </li>
<li>Direct optimize unfairness: <a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~toni/Papers/icml-final.pdf">(Zemel et al., 2013)</a></li>
<li>Adversarial components: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1802.06309.pdf">(Madras et al., 2018)</a></li>
</ul>
<h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h2><ul>
<li>Supervised learning models: SVM, Random Forest, Gaussian Process, LogReg, Adaboost, Naive Bayes.  </li>
<li>Unsupervised: kNN, clustering, domain adaptation, self-supervised embedding.  </li>
<li>Visualization: PCA, T-SNE.  </li>
<li>Regression vs classification.  </li>
<li>Kernel method. <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Kernel_method">Wikipedia</a>  </li>
<li>Decision trees are suitable for nonlinear data <a target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/">Q7</a>.  </li>
<li>Explain boosting, gradient boosting, GBDT.  </li>
<li>CRF is discriminative (measures conditional probability) whereas HMM is generative (measures joint probability).  </li>
</ul>
<h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><ul>
<li>Long Short Term Memory (LSTM)<br>First there are three gates: forget, information, and output.<script type="math/tex; mode=display">f_t = \sigma(W_f [x_t, h_{t-1}] + b_f)</script><script type="math/tex; mode=display">f_i = \sigma(W_i [x_t, h_{t-1}] + b_i)</script><script type="math/tex; mode=display">f_o = \sigma(W_o [x_t, h_{t-1}] + b_o)</script>Then there’s a G gate updating the cell states.  <script type="math/tex; mode=display">g_t = \eta(W_g [x_t, h_{t-1}] + b_g)</script><script type="math/tex; mode=display">C_t = f_t * c_{t-1} + i_t * g_t</script>For output we have: <script type="math/tex">h_t = o_t * \eta(C_t)</script>  </li>
<li>Transformer <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">(Vaswani et al., 2017)</a> Key, Query, Value “gates” per head, multi-head attention, positional encoding.  </li>
<li>Activations. ReLU vs. Sigmoid vs. Tanh.  </li>
<li>Gradient explosion / gradient vanishing. How to address them? (Use ReLU; LSTM gating + gradient clipping etc.)  </li>
<li>Batch normalization <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.03167">(Ioffe and Szegedy, 2015)</a>. Purpose is to address internal covariance shift. Need an additional “scale and shift back” step.  </li>
</ul>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><ul>
<li>Cross-entropy loss or L2 loss?  </li>
<li>Dropout. Explain them.  </li>
<li>Gradient optimizer: GD, SGD.  </li>
<li>Adagrad: adjust the gradients by <script type="math/tex">\frac{1}{diag(\sum grad*grad^T)}</script>. A previous large gradient corresponds to a smaller step.  </li>
<li>Momentum method: use gradients to change the velocity of the weights changing.  <ul>
<li>Nesterov momentum: First jump, then measure gradients, and then make a correction.  </li>
<li>RMSprop: divide the gradient by a running average of its recent magnitude.  </li>
<li>Adam optimizer: a combination of Adagrad and RMSprop.  </li>
<li>See <a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">Hinton’s slides</a> for more explainations.  </li>
</ul>
</li>
<li>Regularization. L1 / L2. L1 (“Least Absolute Shrinkage and Selection Operator”, LASSO) corresponds to a Laplacian prior, with lots of weights around 0, hence encourage sparcity, hence are regarded as having built-in feature selection mechanism. L2 corresponds to a Gaussian prior.   </li>
<li>Handling imbalance classes.  </li>
<li>Handling missing data.  </li>
</ul>
<h2 id="Open-questions"><a href="#Open-questions" class="headerlink" title="Open questions"></a>Open questions</h2><ul>
<li>What’s a favourite paper you recently read?  </li>
<li>What do you plan to do? (short-term / long-term research goals)  </li>
</ul>
<h2 id="NLP-basics"><a href="#NLP-basics" class="headerlink" title="NLP-basics"></a>NLP-basics</h2><ul>
<li>Techniques for keyword normalization? Lemmatization and stemming.  </li>
<li>Techniques for string matching? Levenshtein (i.e., edit distance), soundex, and metaphone.  </li>
<li>Regular Expression (examples in Python).<br><code>.</code> (single dot) matches any character.<br><code>.*</code> matches any character sequences of any length.<br><code>[0-9]</code> matches any one digit.<br><code>[a-z]</code> matches any one lower-case alphabet.<br>Brackets need to be escaped. e.g., <code>\(</code>, <code>\]</code>, if you want to match them.<br><code>\</code> need to be specially taken care of, in Python.  </li>
<li>N-gram: Continuous N words as a bag.  </li>
<li>Zipf’s law. In a corpus, the frequency of a word is inversely proportional to its rank (by frequency).  </li>
</ul>
<h2 id="Information-theory"><a href="#Information-theory" class="headerlink" title="Information theory"></a>Information theory</h2><ul>
<li>Entropy <script type="math/tex">H=-\Sigma_i log(p_i)</script>  </li>
<li>KL divergence  </li>
<li>Jensen-Shannon divergence  </li>
</ul>
<h2 id="NLP-Feature-extractions-and-embeddings"><a href="#NLP-Feature-extractions-and-embeddings" class="headerlink" title="NLP-Feature extractions and embeddings"></a>NLP-Feature extractions and embeddings</h2><ul>
<li>TF-IDF of the term = <script type="math/tex">TF * log \frac{1}{DF}</script>  where term frequency is the term’s frequency, and document frequency is the freq of documents containing the term.  </li>
<li>Popular word embedding methods? GloVe, Word2Vec, FastText, ELMo.  <ul>
<li>GloVe <a target="_blank" rel="noopener" href="https://nlp.stanford.edu/pubs/glove.pdf">(Pennington et al., 2014)</a>: a global log-bilinear regression model trained on word-word co-occurrence counts.  </li>
<li>Word2Vec <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1310.4546.pdf">(Mikolov et al., 2013)</a>: Basically a two-layer networks predicting the context of a word with <script type="math/tex">P=\text{softmax}(w'^T (\sum w_c))</script> (skip-gram), or predict the projected word given its contexts with <script type="math/tex">P=\sum \text{softmax}(w'^T w_c)</script>(CBOW). The output is written as <script type="math/tex">w'</script> in the above equations. Use the first layer output as word embedding.   </li>
<li>FastText <a target="_blank" rel="noopener" href="https://fasttext.cc/">(Facebook AI Research)</a>: An optimized implementation of Word2Vec, but incorporated sub-word information (e.g., including word components of lengths from 3 to 5 as)  </li>
<li>ELMo <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.05365">(Peters et al. 2018)</a>: Use intermediate layers outputs of Bi-LSTMs in additional to word embeddings as new embeddings.  </li>
<li>BERT <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">(Devlin et al. 2018)</a>: Mask 15% of tokens. Predict them based on their contexts using Transformer.  </li>
</ul>
</li>
<li>Topic modeling: LDA.</li>
</ul>
<h2 id="NLP-Parsing"><a href="#NLP-Parsing" class="headerlink" title="NLP-Parsing"></a>NLP-Parsing</h2><ul>
<li>Shift-reduce parser.   </li>
<li>Parsing: dependency vs. constituency parsing.   </li>
</ul>
<h2 id="NLP-Misc"><a href="#NLP-Misc" class="headerlink" title="NLP-Misc"></a>NLP-Misc</h2><ul>
<li>Word sense disambiguation example: Lesk algorithm. Compare the dictionary definition of an ambiguous word with the terms contained in its neighborhood.  </li>
<li>Recommendation system: HITS, PageRank, collaborative filtering.  </li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://ziningzhu.github.io/2018/11/05/2018_11_ML_NLP_interview_prep/" data-id="clprygmat001vnxrq5q390d7p" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/algorithms/" rel="tag">algorithms</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ml-basics/" rel="tag">ml-basics</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/04/15/2019_04_undergraduate_thesis/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Compositional Alignment of Word Embeddings
        
      </div>
    </a>
  
  
    <a href="/2018/08/24/2018_08_asr_overview/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Recognizing the Speech</div>
    </a>
  
</nav>

  
</article>

</section>
        
      </div>
      <footer id="footer">
  
    <aside id="sidebar" class="outer">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/ExplainableAI/" style="font-size: 18.57px;">ExplainableAI</a> <a href="/tags/Information/" style="font-size: 10px;">Information</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/OOD/" style="font-size: 11.43px;">OOD</a> <a href="/tags/PGM/" style="font-size: 10px;">PGM</a> <a href="/tags/Probing/" style="font-size: 10px;">Probing</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Society/" style="font-size: 10px;">Society</a> <a href="/tags/Speech/" style="font-size: 10px;">Speech</a> <a href="/tags/algorithms/" style="font-size: 11.43px;">algorithms</a> <a href="/tags/android/" style="font-size: 10px;">android</a> <a href="/tags/bash/" style="font-size: 10px;">bash</a> <a href="/tags/book-review/" style="font-size: 10px;">book-review</a> <a href="/tags/conferences/" style="font-size: 15.71px;">conferences</a> <a href="/tags/database/" style="font-size: 10px;">database</a> <a href="/tags/drone/" style="font-size: 10px;">drone</a> <a href="/tags/java/" style="font-size: 11.43px;">java</a> <a href="/tags/js/" style="font-size: 10px;">js</a> <a href="/tags/learning/" style="font-size: 10px;">learning</a> <a href="/tags/linguistics/" style="font-size: 12.86px;">linguistics</a> <a href="/tags/ml-basics/" style="font-size: 17.14px;">ml-basics</a> <a href="/tags/philosophy/" style="font-size: 12.86px;">philosophy</a> <a href="/tags/pytorch/" style="font-size: 11.43px;">pytorch</a> <a href="/tags/robotics/" style="font-size: 10px;">robotics</a> <a href="/tags/web/" style="font-size: 14.29px;">web</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/">2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/">2022</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/">2021</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/">2020</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/">2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/">2018</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/">2017</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2016/">2016</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/">2015</a><span class="archive-list-count">1</span></li></ul>
    </div>
  </div>


  
</aside>
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2023 Zining Zhu
      <br>
      Powered by <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a> with <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo-theme-landscape">Landscape</a>.
    </div>
  </div>

</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/research" class="mobile-nav-link">Research</a>
  
    <a href="/teaching" class="mobile-nav-link">Teaching</a>
  
    <a href="/team" class="mobile-nav-link">Team</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" id=""></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


  </div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>