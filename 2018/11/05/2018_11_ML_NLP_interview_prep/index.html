<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>ML / NLP Interviews | Zining&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="This blog compiles some points that might be encountered in ML &#x2F; NLP interviews. Good luck (for myself and to all readers finding jobs).    Statistics   Probability   Causality, Fair ML   Models   Neu">
<meta property="og:type" content="article">
<meta property="og:title" content="ML &#x2F; NLP Interviews">
<meta property="og:url" content="https://ziningzhu.me/2018/11/05/2018_11_ML_NLP_interview_prep/index.html">
<meta property="og:site_name" content="Zining&#39;s Blog">
<meta property="og:description" content="This blog compiles some points that might be encountered in ML &#x2F; NLP interviews. Good luck (for myself and to all readers finding jobs).    Statistics   Probability   Causality, Fair ML   Models   Neu">
<meta property="og:locale">
<meta property="article:published_time" content="2018-11-05T05:12:00.000Z">
<meta property="article:modified_time" content="2021-02-23T22:19:54.640Z">
<meta property="article:author" content="Zining Zhu">
<meta property="article:tag" content="algorithms">
<meta property="article:tag" content="ml-basics">
<meta property="article:tag" content="NLP">
<meta name="twitter:card" content="summary">
<meta name="twitter:creator" content="@https:&#x2F;&#x2F;twitter.com&#x2F;zhuzining">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-84222809-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  <meta name="google-site-verification" content="DI7qg6_y3mwWTdMA-GjrIY60uIS0beb12GxXZZ4A6gw" />

  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <script src="https://use.fontawesome.com/93f0a51ca6.js"></script>
<meta name="generator" content="Hexo 5.4.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Zining&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~zining">About</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a id="nav-twitter-link" class="nav-icon" target="_blank" rel="noopener" href="https://twitter.com/zhuzining">
        
        
          <a id="nav-linkedin-link" class="nav-icon" target="_blank" rel="noopener" href="https://www.linkedin.com/in/zining-zhu-49164496/">
        
        
          <a id="nav-github-link" class="nav-icon" target="_blank" rel="noopener" href="http://github.com/ZiningZhu">
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://ziningzhu.me"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-2018_11_ML_NLP_interview_prep" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/11/05/2018_11_ML_NLP_interview_prep/" class="article-date">
  <time datetime="2018-11-05T05:12:00.000Z" itemprop="datePublished">2018-11-05</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Machine-Learning/">Machine Learning</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      ML / NLP Interviews
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>This blog compiles some points that might be encountered in ML / NLP interviews. Good luck (for myself and to all readers finding jobs).  </p>
<ul>
<li>Statistics  </li>
<li>Probability  </li>
<li>Causality, Fair ML  </li>
<li>Models  </li>
<li>Neural Network  </li>
<li>Training  </li>
<li>Natural Languages Processing specific  <ul>
<li>NLP Basics  </li>
<li>Information theory  </li>
<li>Embedding  </li>
<li>Parsing  </li>
</ul>
</li>
</ul>
<span id="more"></span>
<h2 id="Statistics"><a href="#Statistics" class="headerlink" title="Statistics"></a>Statistics</h2><ul>
<li>Evaluating a model. TP / FP / TN / FN  </li>
<li>Type 1 (false positive) and type 2 errors (false negative).  </li>
<li>Precision (TP / (TP + FP)) vs. Recall (TP / (TP + FN)).  </li>
<li>Sensitivity / specificity.  </li>
<li>F1 score definitions. Micro / macro / weighted.  </li>
<li>ROC: receiver operating characteristic) curve. True positive rate vs. false positive rate at various threshold settings.</li>
<li>AUC: area under (the ROC) curve.  </li>
<li>Bias-variance tradeoff. How to decide whether the model is overfitting, looking from learning plot?  <ul>
<li>High bias: overfit. Solve with more training.  </li>
<li>High variance: too flexible; low generalizability. Solve with (1) regularization (2) bagging algorithm (3) using less and more simple features.</li>
</ul>
</li>
<li>LOOCV, k-fold. They are designed to counteract the bias!   </li>
</ul>
<h2 id="Probability"><a href="#Probability" class="headerlink" title="Probability"></a>Probability</h2><ul>
<li>Bayes Rule: posterior = likelihood * prior / evidence.  </li>
<li>How to compute evidence? Sum them up / integral / etc.  </li>
<li>How to compute posterior when it is almost impossible to do the above steps?  Variational inference (to maximize the variational lower bound) or Monte Carlo sampling (e.g., Metropolis-Hastings algorithm, Gibbs sampling).  </li>
<li>Probability distribution functions (pdf).  </li>
<li>Exponential family distribution functions. Conjugate priors (Beta prior + binomial distribution; Dirichlet prior + multinomial distribution)  </li>
<li>Mutual information <script type="math/tex">I(X, Y) = \sum_X \sum_Y P(X, Y) log\frac{P(X, Y)}{P(X)P(Y)}</script>  </li>
<li>Conditional independence $P(X|Y) = P(Y)$  </li>
<li>Bayesian Network, d-separation.  </li>
<li>Variable elimination.  </li>
</ul>
<h2 id="Causality-and-Fair-Learning"><a href="#Causality-and-Fair-Learning" class="headerlink" title="Causality and Fair Learning"></a>Causality and Fair Learning</h2><ul>
<li>Randomized control trials.  </li>
<li>Rubin-Neyman causal model.  </li>
<li>Traditional deconfounding approaches: residualization, inverse probability weighting.  </li>
<li>Fair Learning: unfairness metrics (demographics parity, equalized opportunity)  </li>
<li>Direct optimize unfairness: <a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~toni/Papers/icml-final.pdf">(Zemel et al., 2013)</a></li>
<li>Adversarial components: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1802.06309.pdf">(Madras et al., 2018)</a></li>
</ul>
<h2 id="Models"><a href="#Models" class="headerlink" title="Models"></a>Models</h2><ul>
<li>Supervised learning models: SVM, Random Forest, Gaussian Process, LogReg, Adaboost, Naive Bayes.  </li>
<li>Unsupervised: kNN, clustering, domain adaptation, self-supervised embedding.  </li>
<li>Visualization: PCA, T-SNE.  </li>
<li>Regression vs classification.  </li>
<li>Kernel method. <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Kernel_method">Wikipedia</a>  </li>
<li>Decision trees are suitable for nonlinear data <a target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2016/09/40-interview-questions-asked-at-startups-in-machine-learning-data-science/">Q7</a>.  </li>
<li>Explain boosting, gradient boosting, GBDT.  </li>
<li>CRF is discriminative (measures conditional probability) whereas HMM is generative (measures joint probability).  </li>
</ul>
<h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><ul>
<li>Long Short Term Memory (LSTM)<br>First there are three gates: forget, information, and output.<script type="math/tex; mode=display">f_t = \sigma(W_f [x_t, h_{t-1}] + b_f)</script><script type="math/tex; mode=display">f_i = \sigma(W_i [x_t, h_{t-1}] + b_i)</script><script type="math/tex; mode=display">f_o = \sigma(W_o [x_t, h_{t-1}] + b_o)</script>Then there’s a G gate updating the cell states.  <script type="math/tex; mode=display">g_t = \eta(W_g [x_t, h_{t-1}] + b_g)</script><script type="math/tex; mode=display">C_t = f_t * c_{t-1} + i_t * g_t</script>For output we have: <script type="math/tex">h_t = o_t * \eta(C_t)</script>  </li>
<li>Transformer <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">(Vaswani et al., 2017)</a> Key, Query, Value “gates” per head, multi-head attention, positional encoding.  </li>
<li>Activations. ReLU vs. Sigmoid vs. Tanh.  </li>
<li>Gradient explosion / gradient vanishing. How to address them? (Use ReLU; LSTM gating + gradient clipping etc.)  </li>
<li>Batch normalization <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1502.03167">(Ioffe and Szegedy, 2015)</a>. Purpose is to address internal covariance shift. Need an additional “scale and shift back” step.  </li>
</ul>
<h2 id="Training"><a href="#Training" class="headerlink" title="Training"></a>Training</h2><ul>
<li>Cross-entropy loss or L2 loss?  </li>
<li>Dropout. Explain them.  </li>
<li>Gradient optimizer: GD, SGD.  </li>
<li>Adagrad: adjust the gradients by <script type="math/tex">\frac{1}{diag(\sum grad*grad^T)}</script>. A previous large gradient corresponds to a smaller step.  </li>
<li>Momentum method: use gradients to change the velocity of the weights changing.  <ul>
<li>Nesterov momentum: First jump, then measure gradients, and then make a correction.  </li>
<li>RMSprop: divide the gradient by a running average of its recent magnitude.  </li>
<li>Adam optimizer: a combination of Adagrad and RMSprop.  </li>
<li>See <a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">Hinton’s slides</a> for more explainations.  </li>
</ul>
</li>
<li>Regularization. L1 / L2. L1 (“Least Absolute Shrinkage and Selection Operator”, LASSO) corresponds to a Laplacian prior, with lots of weights around 0, hence encourage sparcity, hence are regarded as having built-in feature selection mechanism. L2 corresponds to a Gaussian prior.   </li>
<li>Handling imbalance classes.  </li>
<li>Handling missing data.  </li>
</ul>
<h2 id="Open-questions"><a href="#Open-questions" class="headerlink" title="Open questions"></a>Open questions</h2><ul>
<li>What’s a favourite paper you recently read?  </li>
<li>What do you plan to do? (short-term / long-term research goals)  </li>
</ul>
<h2 id="NLP-basics"><a href="#NLP-basics" class="headerlink" title="NLP-basics"></a>NLP-basics</h2><ul>
<li>Techniques for keyword normalization? Lemmatization and stemming.  </li>
<li>Techniques for string matching? Levenshtein (i.e., edit distance), soundex, and metaphone.  </li>
<li>Regular Expression (examples in Python).<br><code>.</code> (single dot) matches any character.<br><code>.*</code> matches any character sequences of any length.<br><code>[0-9]</code> matches any one digit.<br><code>[a-z]</code> matches any one lower-case alphabet.<br>Brackets need to be escaped. e.g., <code>\(</code>, <code>\]</code>, if you want to match them.<br><code>\</code> need to be specially taken care of, in Python.  </li>
<li>N-gram: Continuous N words as a bag.  </li>
<li>Zipf’s law. In a corpus, the frequency of a word is inversely proportional to its rank (by frequency).  </li>
</ul>
<h2 id="Information-theory"><a href="#Information-theory" class="headerlink" title="Information theory"></a>Information theory</h2><ul>
<li>Entropy <script type="math/tex">H=-\Sigma_i log(p_i)</script>  </li>
<li>KL divergence  </li>
<li>Jensen-Shannon divergence  </li>
</ul>
<h2 id="NLP-Feature-extractions-and-embeddings"><a href="#NLP-Feature-extractions-and-embeddings" class="headerlink" title="NLP-Feature extractions and embeddings"></a>NLP-Feature extractions and embeddings</h2><ul>
<li>TF-IDF of the term = <script type="math/tex">TF * log \frac{1}{DF}</script>  where term frequency is the term’s frequency, and document frequency is the freq of documents containing the term.  </li>
<li>Popular word embedding methods? GloVe, Word2Vec, FastText, ELMo.  <ul>
<li>GloVe <a target="_blank" rel="noopener" href="https://nlp.stanford.edu/pubs/glove.pdf">(Pennington et al., 2014)</a>: a global log-bilinear regression model trained on word-word co-occurrence counts.  </li>
<li>Word2Vec <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1310.4546.pdf">(Mikolov et al., 2013)</a>: Basically a two-layer networks predicting the context of a word with <script type="math/tex">P=\text{softmax}(w'^T (\sum w_c))</script> (skip-gram), or predict the projected word given its contexts with <script type="math/tex">P=\sum \text{softmax}(w'^T w_c)</script>(CBOW). The output is written as <script type="math/tex">w'</script> in the above equations. Use the first layer output as word embedding.   </li>
<li>FastText <a target="_blank" rel="noopener" href="https://fasttext.cc/">(Facebook AI Research)</a>: An optimized implementation of Word2Vec, but incorporated sub-word information (e.g., including word components of lengths from 3 to 5 as)  </li>
<li>ELMo <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.05365">(Peters et al. 2018)</a>: Use intermediate layers outputs of Bi-LSTMs in additional to word embeddings as new embeddings.  </li>
<li>BERT <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">(Devlin et al. 2018)</a>: Mask 15% of tokens. Predict them based on their contexts using Transformer.  </li>
</ul>
</li>
<li>Topic modeling: LDA.</li>
</ul>
<h2 id="NLP-Parsing"><a href="#NLP-Parsing" class="headerlink" title="NLP-Parsing"></a>NLP-Parsing</h2><ul>
<li>Shift-reduce parser.   </li>
<li>Parsing: dependency vs. constituency parsing.   </li>
</ul>
<h2 id="NLP-Misc"><a href="#NLP-Misc" class="headerlink" title="NLP-Misc"></a>NLP-Misc</h2><ul>
<li>Word sense disambiguation example: Lesk algorithm. Compare the dictionary definition of an ambiguous word with the terms contained in its neighborhood.  </li>
<li>Recommendation system: HITS, PageRank, collaborative filtering.  </li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://ziningzhu.me/2018/11/05/2018_11_ML_NLP_interview_prep/" data-id="ckljodjxc003q3k1yavcbdrdz" class="article-share-link">Share</a>
      
        <a href="https://ziningzhu.me/2018/11/05/2018_11_ML_NLP_interview_prep/#disqus_thread" class="article-comment-link">Comments</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/NLP/" rel="tag">NLP</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/algorithms/" rel="tag">algorithms</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ml-basics/" rel="tag">ml-basics</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2018/12/31/2018_12_books/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          4+4 Books I loved in 2018
        
      </div>
    </a>
  
  
    <a href="/2018/08/24/2018_08_asr_overview/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Recognizing the Speech</div>
    </a>
  
</nav>

  
</article>


<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>
</section>
        
      </div>
      <footer id="footer">
  
    <aside id="sidebar" class="outer">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/04/23/2022_04_probing_datasets/">&bull; On the data requirements of probing</a>
          </li>
        
          <li>
            <a href="/2022/04/23/2022_04_cxg_probing/">&bull; Neural reality of argument structure constructions</a>
          </li>
        
          <li>
            <a href="/2021/12/28/2021_12_books/">&bull; Cool Books in 2021</a>
          </li>
        
          <li>
            <a href="/2021/11/07/2021_11_task_specific_information/">&bull; Quantifying the Task-Specific Information in Text-Based Classifications</a>
          </li>
        
          <li>
            <a href="/2021/11/07/2021_11_detect_moral_sentiment_change/">&bull; An unsupervised framework for tracing textual sources of moral change</a>
          </li>
        
          <li>
            <a href="/2021/07/20/2021_07_writing_features/">&bull; What do writing features tell us about AI papers?</a>
          </li>
        
          <li>
            <a href="/2021/07/10/2021_07_layerwise_anomaly/">&bull; How is BERT surprised? Layerwise detection of linguistic anomalies</a>
          </li>
        
          <li>
            <a href="/2021/06/10/2021_06_NAACL/">&bull; NAACL 2021 papers</a>
          </li>
        
          <li>
            <a href="/2021/05/27/2021_05_interpretable_NLP/">&bull; Interpretable NLP 2021 winter</a>
          </li>
        
          <li>
            <a href="/2020/12/22/2020_12_books/">&bull; Cool Books in 2020</a>
          </li>
        
          <li>
            <a href="/2020/09/30/2020_09_RSTprobe/">&bull; Examining the rhetorical capacities of neural language models</a>
          </li>
        
          <li>
            <a href="/2020/09/21/2020_09_infoprobe/">&bull; An information theoretic view on selecting linguistic probes</a>
          </li>
        
          <li>
            <a href="/2020/07/10/2020_07_acl2020/">&bull; Trends in ACL 2020</a>
          </li>
        
          <li>
            <a href="/2020/04/12/2020_04_bayes_recap/">&bull; Tested Positive. Do I Have It?</a>
          </li>
        
          <li>
            <a href="/2020/02/26/2020_02_pytorch_questions/">&bull; Pytorch Questions</a>
          </li>
        
      </ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/ExplainableAI/" style="font-size: 18.57px;">ExplainableAI</a> <a href="/tags/GAN/" style="font-size: 10px;">GAN</a> <a href="/tags/Information/" style="font-size: 10px;">Information</a> <a href="/tags/NLP/" style="font-size: 20px;">NLP</a> <a href="/tags/PGM/" style="font-size: 10px;">PGM</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Society/" style="font-size: 10px;">Society</a> <a href="/tags/VAE/" style="font-size: 10px;">VAE</a> <a href="/tags/algorithms/" style="font-size: 11.43px;">algorithms</a> <a href="/tags/android/" style="font-size: 10px;">android</a> <a href="/tags/bash/" style="font-size: 10px;">bash</a> <a href="/tags/book-review/" style="font-size: 17.14px;">book-review</a> <a href="/tags/conferences/" style="font-size: 15.71px;">conferences</a> <a href="/tags/database/" style="font-size: 10px;">database</a> <a href="/tags/drone/" style="font-size: 10px;">drone</a> <a href="/tags/java/" style="font-size: 11.43px;">java</a> <a href="/tags/js/" style="font-size: 10px;">js</a> <a href="/tags/learning/" style="font-size: 11.43px;">learning</a> <a href="/tags/linguistics/" style="font-size: 12.86px;">linguistics</a> <a href="/tags/ml-basics/" style="font-size: 18.57px;">ml-basics</a> <a href="/tags/os/" style="font-size: 10px;">os</a> <a href="/tags/philosophy/" style="font-size: 14.29px;">philosophy</a> <a href="/tags/pytorch/" style="font-size: 11.43px;">pytorch</a> <a href="/tags/robotics/" style="font-size: 10px;">robotics</a> <a href="/tags/teamwork/" style="font-size: 10px;">teamwork</a> <a href="/tags/web/" style="font-size: 14.29px;">web</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Academia/">Academia</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Literature/">Literature</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Machine-Learning/">Machine Learning</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Philosophy/">Philosophy</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Software/">Software</a></li></ul>
    </div>
  </div>


  
</aside>
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 Zining Zhu
      <br>
      Powered by <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a> with <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo-theme-landscape">Landscape</a>.
    </div>
  </div>

</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a target="_blank" rel="noopener" href="https://www.cs.toronto.edu/~zining" class="mobile-nav-link">About</a>
  
</nav>
    
<script>
  var disqus_shortname = 'ziningzhu-me';
  
  var disqus_url = 'https://ziningzhu.me/2018/11/05/2018_11_ML_NLP_interview_prep/';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML" id=""></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>


  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>