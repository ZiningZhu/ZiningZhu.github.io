{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch computation graph\n",
    "1. Memory overflow: what if you don't GC the graph?  \n",
    "2. Gradient zeroing: when to do it, and why necessary?  \n",
    "3. Backward second time: when will this be triggered?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "import psutil\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_memory_usage(label):\n",
    "    process = psutil.Process(os.getpid())\n",
    "    #print(process.memory_info())\n",
    "    mem = process.memory_info().rss / 1024 / 1024\n",
    "    print (\"{} using {:.2f} MB memory!\".format(label, mem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 using 513.50 MB memory!\n",
      "Epoch 20 using 514.36 MB memory!\n",
      "Epoch 30 using 515.20 MB memory!\n",
      "Epoch 40 using 516.04 MB memory!\n",
      "Epoch 50 using 516.88 MB memory!\n",
      "Epoch 60 using 517.72 MB memory!\n",
      "Epoch 70 using 518.56 MB memory!\n",
      "Epoch 80 using 519.40 MB memory!\n",
      "Epoch 90 using 520.24 MB memory!\n",
      "Epoch 100 using 521.08 MB memory!\n"
     ]
    }
   ],
   "source": [
    "def memory_overflow():\n",
    "    \n",
    "    network = nn.Sequential(\n",
    "        nn.Linear(4, 10000), nn.Linear(10000, 10000), nn.Linear(10000, 2))\n",
    "    losses = []\n",
    "    for epoch in range(1, 101):\n",
    "        x = torch.tensor([0,1,2,3]).float().unsqueeze(0)  \n",
    "        y = network(x)\n",
    "        loss = y * y\n",
    "        losses.append(loss.mean())  # A reference to the previous graphs are not freed.\n",
    "        if epoch % 10 == 0:\n",
    "            print_memory_usage(\"Epoch {}\".format(epoch))\n",
    "        \n",
    "gc.collect()\n",
    "memory_overflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 using 521.25 MB memory!\n",
      "Epoch 20 using 521.25 MB memory!\n",
      "Epoch 30 using 521.25 MB memory!\n",
      "Epoch 40 using 521.25 MB memory!\n",
      "Epoch 50 using 521.25 MB memory!\n",
      "Epoch 60 using 521.25 MB memory!\n",
      "Epoch 70 using 521.25 MB memory!\n",
      "Epoch 80 using 521.25 MB memory!\n",
      "Epoch 90 using 521.25 MB memory!\n",
      "Epoch 100 using 521.25 MB memory!\n"
     ]
    }
   ],
   "source": [
    "def no_memory_overflow():\n",
    "    \n",
    "    network = nn.Sequential(\n",
    "        nn.Linear(4, 10000), nn.Linear(10000, 10000), nn.Linear(10000, 2))\n",
    "    losses = []\n",
    "    for epoch in range(1, 101):\n",
    "        x = torch.tensor([0,1,2,3]).float().unsqueeze(0)  \n",
    "        y = network(x)\n",
    "        loss = y * y\n",
    "        losses.append(loss.mean().item())  # The previous graph could be discarded\n",
    "        if epoch % 10 == 0:\n",
    "            print_memory_usage(\"Epoch {}\".format(epoch))\n",
    "        \n",
    "gc.collect()\n",
    "no_memory_overflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Zeroing\n",
    "[This discussion](https://discuss.pytorch.org/t/why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch/4903) could be relevant.  \n",
    "If you don't zero out the gradient, you effectively increased the batch size. This could be useful when you want to train very large batch of data. An example is in [Open NMT](https://github.com/OpenNMT/OpenNMT-py) (in its --accum_count option)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward(): w.grad_=None\n",
      "After backward():  w.grad_=tensor([1.]) (should be 1)\n",
      "Second pass:       w.grad=tensor([2.]) (accumulated to 2)\n",
      "===I zero'ed the grad===\n",
      "Now:               w.grad=tensor([1.]) (back to 1, the correct value)\n"
     ]
    }
   ],
   "source": [
    "def test_gradient_accumulation():\n",
    "    x = torch.ones((1), requires_grad=False)  # requires_grad is default to False\n",
    "    w = torch.ones((1), requires_grad=True)\n",
    "    y = w * x\n",
    "    print (\"Before backward(): w.grad_={}\".format(w.grad))\n",
    "    y.backward()\n",
    "    print (\"After backward():  w.grad_={} (should be 1)\".format(w.grad))\n",
    "    \n",
    "    y = w * x\n",
    "    y.backward()\n",
    "    print (\"Second pass:       w.grad={} (accumulated to 2)\".format(w.grad))\n",
    "    \n",
    "    w.grad.data.zero_()\n",
    "    print (\"===I zero'ed the grad===\")\n",
    "    y = w * x\n",
    "    y.backward()\n",
    "    print (\"Now:               w.grad={} (back to 1, the correct value)\".format(w.grad))\n",
    "    \n",
    "test_gradient_accumulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward second time\n",
    "[This stackoverflow question](https://stackoverflow.com/questions/46774641/what-does-the-parameter-retain-graph-mean-in-the-variables-backward-method) is relevant.  \n",
    "See [this discussion](https://discuss.pytorch.org/t/which-is-freed-which-is-not/8636) for when a node is freed and when it is not.  \n",
    "\n",
    "Basically, this depends on your node types:\n",
    "- The **root node** (`loss`) does not have gradient (dL/dL is not useful). \n",
    "- The **leaves nodes** (`weights`) have their gradients accumulated if you do the second backprop.\n",
    "- Your **intermediate nodes** (all other nodes) are free'd during `backward()` unless you `backward(retain_graph=True)`. This causes the \"Backward Second Time\" error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After backward():  x=tensor([1.]), x.grad=None\n",
      "After backward():  w=tensor([1.], requires_grad=True), w.grad=tensor([2.])\n",
      "After backward():  y=tensor([1.], grad_fn=<MulBackward0>), y.grad=None\n",
      "After backward():  loss=tensor([1.], grad_fn=<MulBackward0>), loss.grad=None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6b7e5cd304a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtest_backprop_second_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-6b7e5cd304a0>\u001b[0m in \u001b[0;36mtest_backprop_second_time\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"After backward():  y={}, y.grad={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"After backward():  loss={}, loss.grad={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtest_backprop_second_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch12/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch12/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "def test_backprop_second_time():\n",
    "    x = torch.ones((1), requires_grad=False)\n",
    "    w = torch.ones((1), requires_grad=True)\n",
    "    y = w * x  # y is not a leaf node\n",
    "    loss = y * y  \n",
    "    loss.backward()\n",
    "    print (\"After backward():  x={}, x.grad={}\".format(x, x.grad))\n",
    "    print (\"After backward():  w={}, w.grad={}\".format(w, w.grad))\n",
    "    print (\"After backward():  y={}, y.grad={}\".format(y, y.grad))\n",
    "    print (\"After backward():  loss={}, loss.grad={}\".format(loss, loss.grad))\n",
    "    loss.backward()\n",
    "    \n",
    "test_backprop_second_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== backward() with retain_graph ===\n",
      "After backward():  x=tensor([1.]), x.grad=None\n",
      "After backward():  w=tensor([1.], requires_grad=True), w.grad=tensor([2.])\n",
      "After backward():  y=tensor([1.], grad_fn=<MulBackward0>), y.grad=None\n",
      "After backward():  loss=tensor([1.], grad_fn=<MulBackward0>), loss.grad=None\n",
      "=== backward() again ===\n",
      "After backward():  x=tensor([1.]), x.grad=None\n",
      "After backward():  w=tensor([1.], requires_grad=True), w.grad=tensor([4.]) (you can see gradient accumulation too)\n",
      "After backward():  y=tensor([1.], grad_fn=<MulBackward0>), y.grad=None\n",
      "After backward():  loss=tensor([1.], grad_fn=<MulBackward0>), loss.grad=None\n"
     ]
    }
   ],
   "source": [
    "def test_backprop_second_time():\n",
    "    x = torch.ones((1), requires_grad=False)\n",
    "    w = torch.ones((1), requires_grad=True)\n",
    "    y = w * x  # y is not a leaf node\n",
    "    loss = y * y  \n",
    "    loss.backward(retain_graph=True)\n",
    "    print (\"=== backward() with retain_graph ===\")\n",
    "    print (\"After backward():  x={}, x.grad={}\".format(x, x.grad))\n",
    "    print (\"After backward():  w={}, w.grad={}\".format(w, w.grad))\n",
    "    print (\"After backward():  y={}, y.grad={}\".format(y, y.grad))\n",
    "    print (\"After backward():  loss={}, loss.grad={}\".format(loss, loss.grad))\n",
    "    loss.backward()\n",
    "    print (\"=== backward() again ===\")\n",
    "    print (\"After backward():  x={}, x.grad={}\".format(x, x.grad))\n",
    "    print (\"After backward():  w={}, w.grad={} (you can see gradient accumulation too)\".format(w, w.grad))\n",
    "    print (\"After backward():  y={}, y.grad={}\".format(y, y.grad))\n",
    "    print (\"After backward():  loss={}, loss.grad={}\".format(loss, loss.grad))\n",
    "    \n",
    "test_backprop_second_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
